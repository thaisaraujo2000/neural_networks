{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questão 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Conforme Géron(2019), o algoritmo de otimização conhecido como Gradiente Descendente consiste em realizar ajustes iterativos nos parâmetros de um modelo com o intuito de minimizar uma função custo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Série de Taylor:\n",
    "$$E((w(n) + \\Delta w(n)) = E(w(n)) + \\nabla E^T(w(n))\\Delta w(n) + \\frac{1}{2}\\Delta w^T(n)H(w(n))\\Delta w(n) + O(||\\Delta w||^3)$$\n",
    "\n",
    "Método do gradiente descendente:\n",
    "$w(n+1) = w(n) - \\eta g(w(n))$\n",
    "\n",
    "Substituir $\\Delta w(n) = -\\eta g(w(n))$ na série de Taylor permite obter uma aproximação para $E(w(n+1))$, a partir de $E(w(n))$. Como o método do gradiente descendente atualiza $w(n)$ para $w(n+1)$ de acordo com a regra $w(n+1) = w(n) - \\eta g(w(n))$, queremos chegar em uma expressão para $E(w(n+1))$ que dependa apenas de $w(n)$ e $g(w(n))$. Ao fazer essa substituição, podemos cancelar os termos de ordem superior a 1 em $\\Delta w(n)$ e obter uma aproximação linear para $E(w(n+1))$ em termos de $E(w(n))$, $g(w(n))$ e $\\eta$. Isso nos permite aplicar o método do gradiente descendente para minimizar $E(w)$, atualizando iterativamente os pesos até convergir para um mínimo.\n",
    "\n",
    "Dessa forma, substituindo $\\Delta w(n) = -\\eta g(w(n))$ na série de Taylor, temos:\n",
    "\n",
    "$E(w(n+1)) = E(w(n) - \\eta g(w(n)))$ \n",
    "\n",
    "$E(w(n+1)) \\approx E(w(n)) + \\nabla E^T(w(n))(-\\eta g(w(n))) + \\frac{1}{2}(-\\eta g^T(w(n)))H(w(n))(-\\eta g(w(n))) + O(\\eta^3)$\n",
    "\n",
    "$E(w(n+1)) = E(w(n)) - \\eta\\nabla E^T(w(n))g(w(n)) + \\frac{1}{2}\\eta^2g^T(w(n))H(w(n))g(w(n)) + O(\\eta^3)$\n",
    "\n",
    "\n",
    "\n",
    "Quando utilizamos o método do gradiente descendente, o objetivo é minimizar a função de erro $E(w)$, ou seja, queremos encontrar o mínimo da função. Para isso, buscamos a direção de maior descida da função, que é oposta à direção do gradiente $g(w(n))$. A regra de atualização do método do gradiente descendente é simplesmente atualizar os pesos $w(n)$ na direção do gradiente, com um fator de aprendizado $\\eta$ que determina o tamanho do passo de atualização.\n",
    "Assim, a partir da aproximação acima, podemos aproximar $E(w(n+1))$ por $E(w(n)) - \\eta\\nabla E^T(w(n))g(w(n))$. Para isso, basta igualar essa aproximação a $E(w(n+1))$ e manipular a expressão para chegar em uma expressão para $w(n+1)$. Isso pode ser feito da seguinte forma:\n",
    "\n",
    "$E(w(n+1))$ = $E(w(n)) - \\eta\\nabla E^T(w(n))g(w(n))$\n",
    "\n",
    "Como queremos encontrar a atualização para os pesos $w$, basta pegar a derivada em relação a $w(n+1)$ em ambos os lados:\n",
    "\n",
    "$\\frac{\\partial E(w(n+1))}{\\partial w(n+1)} \\approx \\frac{\\partial E(w(n))}{\\partial w(n+1)} - \\frac{\\eta \\nabla E^T(w(n))g(w(n))}{\\partial w(n+1)}$\n",
    "\n",
    "Note que a derivada da expressão à direita é zero, uma vez que o gradiente local $g(w(n))$ não depende de $w(n+1)$. Logo, temos:\n",
    "\n",
    "$\\frac{\\partial E(w(n+1))}{\\partial w(n+1)} \\approx \\frac{\\partial E(w(n))}{\\partial w(n+1)}$\n",
    "\n",
    "Agora, podemos substituir $w(n+1)$ por $w(n) - \\eta g(w(n))$ para obter:\n",
    "\n",
    "$\\frac{\\partial E(w(n+1))}{\\partial w(n+1)} \\approx \\frac{\\partial E(w(n))}{\\partial w(n+1)} = \\frac{\\partial E(w(n))}{\\partial w(n)} \\cdot \\frac{\\partial w(n)}{\\partial w(n+1)} $\n",
    "$ = -\\nabla E^T(w(n))$\n",
    "\n",
    "Assim, chegamos em:\n",
    "\n",
    "$-\\nabla E^T(w(n+1)) \\approx -\\nabla E^T(w(n))$\n",
    "\n",
    "Ou seja:\n",
    "\n",
    "$\\nabla E^T(w(n+1)) \\approx \\nabla E^T(w(n))$\n",
    "\n",
    "E, finalmente, temos a regra de atualização do método do gradiente descendente:\n",
    "\n",
    "$$w(n+1) = w(n) - \\eta g(w(n))$$\n",
    "\n",
    "Essa regra de atualização nos permite atualizar os pesos da rede neural em direção ao mínimo da função de erro $E(w)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referência"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GERON, Aurélien. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition. O'Reilly Media, Inc., 2019. ISBN: 9781492032649. Disponível em: https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/. Acesso em: 02 abr. 2023."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
