{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Questão 1**\n",
    "\n",
    "As  métricas  de  distancia  entre  vetores  são  aplicadas  nos  estudos  de aprendizagem  de máquina  como  medidas  de  similaridade/dissimilaridade  entre  vetores  que  representam  padrões. Apresente um estudo sobre as seguintes distancias: \n",
    "- Distância Euclidiana \n",
    "- Métrica de Minkowski\n",
    "- Distancia de Mahalanobis\n",
    "- Coeficiente de Correlação de Pearson \n",
    "- Similaridade de Cosseno\n",
    "\n",
    "Apresente  neste  estudo  aplicações  de  métricas  de  distância  em  problemas  de livre escolha. Sugestão: Classificação de padrões, Clustering, Reconhecimento de padrões, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Distância Euclidiana**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De acordo com Jiawei et al. (2011), a distância Euclidiana é uma das métricas mais comuns usadas em aprendizado de máquina para medir a similaridade entre dois vetores. Ela é definida como a raiz quadrada da soma dos quadrados das diferenças entre cada par de elementos correspondentes nos vetores, como mostra a equação 1. Essa métrica é baseada no Teorema de Pitágoras, e pode ser visualizada como a distância entre dois pontos em um espaço euclidiano.\n",
    "\n",
    "Equação 1.\n",
    "Sejam dois vetores $i = (x_1, x_2, ..., x_n)$ e $j = (y_1, y_2, ..., y_n)$. A distância euclidiana $d(i,j)$ entre esses dois vetores é dada por:\n",
    "\n",
    "$d(i,j) = \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + ... + (x_n - y_n)^2}$\n",
    "\n",
    "Outra métrica popular para calcular a distância (ou a similaridade) entre dois vetores é a distância de Manhattam, a qual recebe esse nome por causa da distância em blocos entre dois pontos da cidade. A equação 2 descreve a distância $d(i,j)$ de Manhattan.\n",
    "\n",
    "Equação 2. \n",
    "\n",
    "$d(i,j) = |(x_1 - y_1)| + |(x_2 - y_2)| + ... + |(x_n - y_n)|$\n",
    "\n",
    "Ainda de acordo com o autor, essas duas métricas satisfazem as propriedades matemáticas da não-negatividade, da identidade dos indisceníveis, da simetria e da desigualdade triangluar, as quais estão descritas abaixo.\n",
    "\n",
    "- Não-negatividade: $d(i,j) \\geq 0$ - A distância é sempre é maior do que zero.\n",
    "- Identidade dos indiscerníveis: $d(i,i) = 0$ - A distância de um objeto para ele mesmo é sempre zero.\n",
    "- Simetria: $d(i,j) = d(j,i)$ - A distância é uma função simétrica.\n",
    "- Desigualdade triangular:  $d(i,j) \\leq d(i,k) + d(k,j)$ - A distância direta entre $i$ e $j$ é sempre menor do que a distância entre eles passando pelo ponto $k$\n",
    "\n",
    "A figura 1 mostra a diferença do cálculo entre a distância euclidiana e distância de Manhattan.\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='./images/euclidean.png'>\n",
    "    \n",
    "    Figura 1. Distância Euclidiana vs Distância de Manhattan. Fonte: dearC (2019).\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Métrica de Minkowski**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De acordo com Jiawei et al. (2011), a métrica de Minkowski é uma generalização das distâncias Euclidiana e Manhattan, a qual pode ser definida pela equação 3.O parâmetro $h$ pode ser alterado para permitir diferentes graus de ênfase em grandes diferenças de valor. Assim, essa métrica possui alguns casos especiais. Por exemplo, quando $h=1$, a métrica de Minkowski é chamada de distância Manhattan, ao passo que quando $h=2$, torna-se a distância Euclidiana.\n",
    "\n",
    "Equação 3.\n",
    "\n",
    "$d(i,j) = \\sqrt[h]{|(x_1 - y_1)|^h + |(x_2 - y_2)|^h + ... + |(x_n - y_n)|^h}$\n",
    "\n",
    "Quando $h \\rightarrow \\infty$, obtêm-se a chamada distância suprema, também conhecida como distância de Chebychev. A "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Distância de Mahalanobis**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De acordo com DAMIANI (2021), a distância de Mahalanobis é uma medida de distância utilizada em aprendizado de máquina que leva em conta a correlação entre as diferentes variáveis que compõem um conjunto de dados. Ela é útil quando os diferentes atributos estão correlacionados, o que pode resultar em uma superestimação da distância Euclidiana entre dois pontos de dados.\n",
    "\n",
    "Ainda segundo o autor, ela é definida como a distância entre dois pontos de dados normalizada pela matriz de covariância dos dados. A matriz de covariância leva em conta a correlação entre os diferentes atributos, o que permite que a distância de Mahalanobis capture melhor as diferenças entre dois pontos de dados, mesmo que possuam muitas dimensões. A equação 4 mostra a definição da distância de Mahalanobis. \n",
    "\n",
    "Equação 4. \n",
    "\n",
    "$d(i,j) = \\sqrt{(\\overrightarrow{x} - \\overrightarrow{y})^T S^{-1}(\\overrightarrow{x} - \\overrightarrow{y})}$\n",
    "\n",
    "Onde $S^{-1}$ é a matriz inversa da covariância.\n",
    "\n",
    "Uma aplicação interessante foi descrita no trabalho de DAMIANI (2021), a qual o autor utilizou a distância de Mahalanobis para a detecção de rompimentos de linhas de injeção de gás. Assim, essa medida foi utilizada com o intuito de reconhecer padrões e realizar o agrupamento de bancos de dados, resultando na identificação correto do rompimento em 9 dos 10 casos e apresentando resultados de falso positivos em 9 dos 33 desvios de processo estudados."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Coeficiente de Correlação de Pearson**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De acordo com MUKAKA (2012), o Coeficiente de Correlação de Pearson ($\\rho$), também conhecido como Correlação de Pearson ou simplesmente Correlação, é uma medida de correlação linear entre duas variáveis quantitativas. Ele indica o grau e a direção da relação linear entre as variáveis, variando de -1 a 1. O valor -1 indica uma correlação negativa perfeita, 0 indica ausência de correlação e 1 indica uma correlação positiva perfeita. A equação 5 mostra a definição de $\\rho$.\n",
    "\n",
    "$\\rho = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\cdot \\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2}}$\n",
    "\n",
    "onde $\\bar{x}$ e $\\bar{y}$ são as métricas aritméticas de cada variável.\n",
    "\n",
    "O Coeficiente de Correlação de Pearson é amplamente utilizado em estatística, ciência dos dados e aprendizado de máquina. Ele é usado para avaliar a relação entre duas variáveis, bem como para identificar variáveis importantes para modelagem e previsão."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Similaridade de Cosseno**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De acordo com JIAWEI (2001), a similaridade de cosseno é uma medida de distância entre dois vetores em um espaço multidimensional. Ela é amplamente utilizada em ciência de dados, aprendizado de máquina e processamento de linguagem natural. A equação 6 define a similariadde de cosseno $sim_{cos}(i,j)$.\n",
    "\n",
    "Equação x.\n",
    "$sim_{cos}(i,j) = cos(\\theta) = \\frac{i \\cdot j}{||i|| ||j||}$\n",
    "\n",
    "onde $||i||$ é a norma euclidiana do vetor $i = (x_1, x_2, ..., x_n)$\n",
    "\n",
    "Assim, caso os dois vetores sejam perfeitamente semelhantes, o cosseno do ângulo será igual 1 ($\\theta=0$), ao passo que se os vetores forem ortogonais, o cosseno será igual a 0 ($\\theta=90$). Se os vetores estiverem em direções opostas, o cosseno será igual a -1 ($\\theta=180$). A similaridade de cosseno é útil em muitos cenários, como na análise de dados de texto, onde os vetores são usados para representar palavras ou documentos, além de ser bastante utilizada para encontrar palavras ou documentos semelhantes em um conjunto de dados.\n",
    "\n",
    "Um exemplo de aplicação desse último caso é a ferramenta chamada [Milvus](https://milvus.io/), a qual consiste em um banco de vetores construído para busca por similaridade. Assim, o usuário armazena os vetores no banco com algumas colunas de dados e constrói um espaço vetorial a partir de uma das métricas informadas na criação da base de vetores. Entre as métricas disponíveis, destacam-se a similaridade de cosseno, a distância euclidiana, a distância de Jaccard, de Tanimoto e de Haming. Quando uma busca é realizada no banco, o Milvus calcula a distância entre o vetor de entrada e os armazenados, retornando uma quantidade pré-definida dos vetores mais próximos. Esse vetores podem ser, por exemplo, documentos jurídicos, mensagens, imagens, aúdios, vídeos, entre muitas outras coisas. Outra característica interessante é a possibilidade de criação de clusters. O usuário informa quantos clusters serão criados e o próprio algoritmo agrupa os vetores armazenados com base na métrica estabelecida. Assim, quando uma busca for realizada, ao invés do algoritmo calcular a disTancia entre cada vetor, ele irá calcular a distância do centro do cluster e retornar os vetores que se encontram mais próximos do centro."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Referências**\n",
    "\n",
    "dearC. Log Book — Guide to Distance Measuring Approaches for K- Means Clustering, 2019. Disponível em: https://towardsdatascience.com/log-book-guide-to-distance-measuring-approaches-for-k-means-clustering-f137807e8e21. Acesso em 01 de abr. 2023.\n",
    "\n",
    "DAMIANI, E. J. Uso da Distância de Mahalanobis para a Detecção de Rompimentos em Linhas de Injeção de Gás, 2021. Disponível em: https://lume.ufrgs.br/bitstream/handle/10183/235773/001136697.pdf?sequence=1&isAllowed=y. Acesso em: 01 de abr. 2023.\n",
    " \n",
    "\n",
    "JIAWEI, H. et al. Data Mining: Concepts and Techniques. Morgan Kaufmann Publishers. 3ª edição. 2011.\n",
    "\n",
    "MUKAKA, MM. A guide to appropriate use of Correlation coefficient in medical research, 2012. PMC: [3576830](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3576830/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
