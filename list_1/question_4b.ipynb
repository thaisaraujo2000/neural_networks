{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questão 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Conforme Goodfellow, Bengio e Courville (2016), o método de otimização conhecido como gradiente descendente apresenta limitações ao não explorar adequadamente as informações de curvatura contidas na matriz Hessiana. Nesse contexto, surge o método de Newton como uma alternativa que utiliza a informação da Hessiana para encontrar a direção de busca mais promissora, atualizando a solução através de uma série de passos baseados em informações de primeira e segunda ordem da função objetivo. Em contrapartida ao gradiente descendente, o método de Newton possui maior capacidade para explorar a curvatura da função, permitindo encontrar soluções mais rapidamente e com menos iterações, evitando caminhos que levam a vales estreitos e longos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Série de Taylor:\n",
    "$$E((w(n) + \\Delta w(n)) = E(w(n)) + \\nabla E^T(w(n))\\Delta w(n) + \\frac{1}{2}\\Delta w^T(n)H(w(n))\\Delta w(n) + O(||\\Delta w||^3)$$\n",
    "\n",
    "Método de Newton:\n",
    "$w(n+1) ≈ w(n) + H^{-1}(w(n))g(w(n))$\n",
    "\n",
    "Usando a série de Taylor de segunda ordem em torno de $w(n+1)$, temos:\n",
    "\\begin{align*}\n",
    "E(w(n+1)) &\\approx E(w(n)) + \\nabla E^T(w(n))(w(n+1) - w(n)) + \\frac{1}{2}(w(n+1) - w(n))^T H(w(n))(w(n+1) - w(n)) \\\\\n",
    "&= E(w(n)) + \\nabla E^T(w(n))(w(n+1) - w(n)) + \\frac{1}{2}(w(n+1) - w(n))^T H(w(n))(w(n+1) - w(n)) \\\\\n",
    "&= E(w(n)) + \\nabla E^T(w(n))(\\Delta w(n)) + \\frac{1}{2}(\\Delta w(n))^T H(w(n))(\\Delta w(n))\n",
    "\\end{align*}\n",
    "\n",
    "Queremos encontrar o mínimo dessa função, portanto, devemos igualar a derivada da função em relação a $\\Delta w(n)$ a zero. Assim, diferenciando em relação a $\\Delta w(n)$ e igualando a zero, temos:\n",
    "\\begin{align*}\n",
    "\\nabla E(w(n)) + H(w(n))\\Delta w(n) &= 0 \\\\\n",
    "\\Delta w(n) &= -H^{-1}(w(n))\\nabla E(w(n))\n",
    "\\end{align*}\n",
    "\n",
    "Substituindo em $w(n+1) \\approx w(n) + \\Delta w(n)$, obtemos:\n",
    "\\begin{align*}\n",
    "w(n+1) &\\approx w(n) - H^{-1}(w(n))\\nabla E(w(n))\n",
    "\\end{align*}\n",
    "\n",
    "Para determinar se o ponto com gradiente zero é um ponto de mínimo, máximo ou ponto de sela, precisamos analisar os autovalores da matriz Hessiana $H(w)$. Considerando que $g(w)=0$ em um determinado ponto, temos que $\\nabla E(w)=H(w)\\Delta w=0$. Portanto, $\\Delta w$ é um autovetor associado a um autovalor nulo. Se todos os autovalores de $H(w)$ forem positivos, então temos um ponto de mínimo. Se todos os autovalores forem negativos, então temos um ponto de máximo. Caso contrário, temos um ponto de sela."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referência"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GOODFELLOW, Ian; BENGIO, Yoshua; COURVILLE, Aaron. Deep Learning. Cambridge, MA: MIT Press, 2016."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
