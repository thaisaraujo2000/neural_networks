{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questão 5\n",
    "\n",
    "Apresente um estudo dos seguintes algoritmos de otimização:\n",
    "\n",
    "    - Descida do Gradiente Estocástico (SGD)\n",
    "    - AdaGrad\n",
    "    - RMSProp\n",
    "    - Adam \n",
    "    \n",
    "Estes métodos ou otimizadores são utilizados no processo de aprendizagem de redes neurais/deep learning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os algoritmos otimizadores são métodos utilizados para ajustar os parâmetros de uma rede neural, durante o processo de treinamento, com o objetivo de minimizar a função de perda e melhorar o desempenho da rede. Um dos algoritmos clássicos é o <i>Gradient Descent</i> (GD), o qual utiliza o cálculo do gradiente da função custo para ajustar os pesos da rede neural em direção ao mínimo global, apesar de que as vezes ele pode ficar preso em um mínimo local. Outros algoritmos bastante famosos são: o SGD, o Adagrad, o RMSProp e o Adam, os quais serão explorados mais detalhadamente a seguir."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Descida do Gradiente Estocástico:\n",
    "\n",
    "A Descida do Gradiente Estocástico (Stochastic Gradient Descent ou SGD) é uma extensão do algoritmo Descida do Gradiente (GD, em inglês) e consiste um algoritmo de otimização que usa amostras aleatórias dos dados de treinamento para estimar o gradiente da função de custo [5]. A diferença básica é que SG utiliza todo o conjunto de treinamento para atualizar os pesos e os viéses dos neurônios, ao passo que o SGD utiliza um fração aleatória dos dados para realizar a atualização dos parâmetros, também conhecidos como mini-lotes. O tamanho dessa fração é definido pelo hiperparâmetro chamado de <i>batch size</i> [8]. Assim, o termo estocástico do SGD faz referência aos dados que são escolhidos aleatoriamente para o lote. A figura 1 nos auxilia a perceber qual o papel dos algoritmos de otimização, ou seja, eles buscam a cada iteração entrar o caminho que aponta para o mínimo global da função.\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='./images/sgd_way.png'>\n",
    "    Figure 1: Gradient descent em uma superfície de erro (2 parâmetros). Fonte: Fraçois Chollet (2018) [5].\n",
    "</p>\n",
    "\n",
    "\n",
    "De acordo com o autor, existem muitas variações, dado que existe o que utiliza os mini-lotes, o que utiliza apenas um dado a cada atualização de parâmetros e existem ainda o que utilizam o chamado Momentum. Essa técnica ajuda a suavizar as flutuações do gradiente e a manter o movimento em direção ao mínimo global da função de perda, mesmo que o gradiente varie bastante em diferentes iterações do treinamento. Isso é importante, pois caso o SGD esteja utilizando uma taxa de aprendizagem baixa, o algoritmo pode ficar preso em um mínimo local e não convergir para o mínimo global da função, como mostra a figura 2. Com isso em vista, o Momentum ajuda o algoritmo a não ficar preso em mínimos locais ou em platôs da função de perda, além de ajudar a acelerar a convergência do treinamento, especialmente em casos em que a superfície da função de perda é muito irregular ou possui muitos mínimos locais.\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='./images/locals.png'>\n",
    "    Figura 2: Gradient descent em uma superfície de erro (2 parâmetros). Fonte: Fraçois Chollet (2018)[5].\n",
    "</p>\n",
    "\n",
    "A função matemática que define o SGD está mostrada na equação 1.\n",
    "\n",
    "Equação 1. Algoritmo SGD\n",
    "\n",
    "1. $w(t+1) = w(t) - \\eta \\nabla L (w(t), x(i), y(i))$, onde:\n",
    "\n",
    "- w(t) é o vetor de pesos da rede neural no momento t;\n",
    "- $\\eta$ é a taxa de aprendizado, que controla a magnitude das atualizações dos pesos;\n",
    "- $\\nabla L (w(t), x(i), y(i))$ é o gradiente da função de perda L em relação aos pesos w no ponto (x(i), y(i)), que é estimado usando um único exemplo de treinamento (modo estocástico);\n",
    "- t é o índice da iteração do SGD;\n",
    "- w(t+1) é o vetor de pesos atualizado após a iteração t."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Adagrad\n",
    "\n",
    "De acord com a figura 3, é possível observar que o algoritmo SGD desce rapidamente a encost a mais íngreme e depois desce mais lentamente para o fundo do vale. Tendo isso em vista, a proposta do Adagrad é detectar isso desde o início e tentar apontar um pouco mais em direção ao ótimo global. Para conseguir isso, o algoritmo Adagrad [2] reduz o vetor gradiente ao longo das dimensões mais íngremes, como mostra equação 2.\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='./images/adagrad.png'>\n",
    "    Figura 3: Adagrad vs Gradient Descent.\n",
    "</p>\n",
    "\n",
    "Equation 2. Algoritmo AdaGrad\n",
    "1. $s \\leftarrow s + \\nabla_\\theta J(\\theta)\\otimes\\nabla_\\theta J(\\theta)$\n",
    "2. $\\theta \\leftarrow \\theta - \\eta\\nabla_\\theta J(\\theta)⊘\\sqrt{s + \\epsilon}$\n",
    "\n",
    "A primeira equação mostra que o vetor s acumula o quadrado do gradiente dos vetores. Em outras palavras, cada elemento $s_i$ acumula o quadrado da derivada parcial da função custo em relação ao parâmetro $\\theta_i$. Já a segunda equação mostra um passo que é quase idêntico ao <i>Gradient Descent</i>, mas com a diferença do vetor ser escalado por um fator de $\\sqrt{s + \\epsilon}$\n",
    "\n",
    "Em resumo, esse algoritmo adapta a taxa de aprendizado (learning rate) para cada parâmetro, levando em consideração a magnitude dos gradientes anteriores. Ele tende a reduzir a taxa de aprendizado para parâmetros que têm grandes gradientes e aumentá-la para aqueles que têm gradientes pequenos. Essa taxa de aprendizado adaptative (adaptative learning rate) possui a vantagem de ter mais independência do hiperparâmetro learning rate que o usuário coloca no treinamento da rede.\n",
    "\n",
    "De forma geral, esse algoritmo costuma performar em bem em problemas quadráticos ou em problemas de regressão, por exemplo. Contudo, em rede neurais, o Adagrad geralmente acaba diminuindo tanto a taxa de aprendizagem que o algoritmo para completamente antes de chegar o ótimo global. Então, a literatura [4] consultada para este estudo não recomenda a utilização deste para o treinamento de redes neurais, problema este que pode ser observado nas experimentações realizadas nas questões sub-sequentes dessa lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# adagrad implementation\n",
    "def adagrad(x, y, w, b, alpha, num_iter):\n",
    "    # initialize the gradient\n",
    "    grad_w = np.zeros(w.shape)\n",
    "    grad_b = 0\n",
    "    # initialize the sum of square gradient\n",
    "    sum_w = np.zeros(w.shape)\n",
    "    sum_b = 0\n",
    "    # initialize the loss\n",
    "    loss = np.zeros(num_iter)\n",
    "    for i in range(num_iter):\n",
    "        # calculate the gradient\n",
    "        grad_w = -2 * np.dot(x.T, (y - np.dot(x, w) - b))\n",
    "        grad_b = -2 * np.sum(y - np.dot(x, w) - b)\n",
    "        # calculate the sum of square gradient\n",
    "        sum_w += grad_w ** 2\n",
    "        sum_b += grad_b ** 2\n",
    "        # update the parameters\n",
    "        w = w - alpha * grad_w / (np.sqrt(sum_w) + 1e-8)\n",
    "        b = b - alpha * grad_b / (np.sqrt(sum_b) + 1e-8)\n",
    "        # calculate the loss\n",
    "        loss[i] = np.sum((y - np.dot(x, w) - b) ** 2) / x.shape[0]\n",
    "    return w, b, loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. RMSProp\n",
    "\n",
    "O RMSProp (Root Mean Square Propagation ou Propagação da Raiz Quadrática Média) é um algoritmo de otimização que adapta a taxa de aprendizado para cada parâmetro, levando em consideração a média dos gradientes quadrados anteriores. Isso ajuda a lidar com gradientes de diferentes magnitudes. Além disso, o algoritmo RMESprop inclui um termo de amortecimento (damping term) para controlar a taxa de aprendizado global, como é possível observar na equação 3.\n",
    "\n",
    "Equação 3. Algoritmo do RMSProp\n",
    "1. $s \\leftarrow \\beta s + (1 - \\beta)\\nabla_\\theta J(\\theta)\\otimes\\nabla_\\theta J(\\theta)$\n",
    "2. $\\theta \\leftarrow \\theta - \\eta\\nabla_\\theta J(\\theta)⊘\\sqrt{s + \\epsilon}$\n",
    "\n",
    "Ou seja, o RMSProp atribui pesos maiores para os valores mais novos, ao passo que os valores mais antigos vão decaindo exponencialmente pelo valor de $\\beta$. Dessa forma, s é atualizado opr meio de uma média movel exponencial, amortecendo os valores mais antigos até que eles não tenham uma contribuição significativa [3].\n",
    "\n",
    "Exceto em algumas situações, o RMSProp apresenta uma performance melhor do que o algortimo Adagrad, tendo sido o algoritmo de otimização preferido em muitas pesquisas até o surgimento do Adam [4].\n",
    "\n",
    "Nota: Esse algoritmo foi criado por Geoffrey Hinton and Tijmen Teleman in 2012 e foi apresentado por Hinton no curso de redes neurais na Coursera (slides: https://homl.info/57; video: https://homl.info/58). Assim, como os autores não escreveram um artigo sobre o algoritmo, os pesquisadores normalmente citam \"slide 29 na lição 6\" em seus artigos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmsprop implementation\n",
    "def rmsprop(params, sqrs, lr, gamma, batch_size, eps=1e-6):\n",
    "    for param, sqr in zip(params, sqrs):\n",
    "        sqr[:] = gamma * sqr + (1 - gamma) * param.grad.square()\n",
    "        param[:] -= lr * param.grad / (sqr / batch_size + eps).sqrt()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Adam\n",
    "\n",
    "O Adam (Adaptive Moment Estimation ou Estimativa de Momento Adaptativo) [7] é um algoritmo de otimização que combina o RMSProp com o momento (Momentum). O momento rastreia a mécia movel exponencial dos gradientes passados e ajuda a acelerar a convergência em direções consistentes, enquanto o RMSProp rastreia a média móvel exponencial dos gradientes quadráticos passados e adapta a taxa de aprendizado para cada parâmetro. O Adam também inclui um termo de viés (bias term) para corrigir os momentos iniciais, que podem ser muito baixos no início do treinamento [1]. A partir da equação 4, é possível notar que algoritmo do otimizador Adam é, de fato, bastante similar aos algoritmos de Momentum e RMSProp.\n",
    "\n",
    "Equação 4. Algoritmo do Adam\n",
    "1. $m \\leftarrow \\beta_1 m - (1-\\beta_1)\\nabla_\\theta J(\\theta)$\n",
    "2. $s \\leftarrow \\beta_2 s + (1 - \\beta_2)\\nabla_\\theta J(\\theta)\\otimes\\nabla_\\theta J(\\theta)$\n",
    "3. $\\hat{m} \\leftarrow \\frac{m}{1 - \\beta_1^t}$\n",
    "4. $\\hat{s} \\leftarrow \\frac{s}{1 - \\beta_2^t}$\n",
    "5. $\\theta \\leftarrow \\theta - \\eta\\nabla_\\theta J(\\theta)⊘\\sqrt{s + \\epsilon}$\n",
    "\n",
    "O passo 1 possui a diferença de que o algoritmo do Momentum calcula uma soma descrescente exponencialmente, ao passo que o Adam calcula a média descrescente exponencialmente por um fator de ($1 - \\beta_1$) vezes a soma decrescente. \n",
    "\n",
    "O Adam é considerado um dos melhores algoritmos de otimização atualmente e é frequentemente usado em redes neurais profundas. Os passos 2 e 5 são exatamente iguais aos do RMSProp e os passos 3 e 4 servem para aumentar os valores de s e m, uma vez que eles são inicializados com o valor 0.\n",
    "\n",
    "A figura 4 mostra uma comparação entre os algoritmos SGD, RMSProp e Adam e à primeira vista, já é possível observar que o último é bem menos ruidoso do que os dois primeiros. Após uma observação mais atenta, é possível notar, no início, o SGD se encaminha para uma direção errada, ao passo que o RMSProp se encaminha para a direção correta. Contudo, o RMSprop sofre de ruído da mesma forma que o SGD, então ele oscila em torno do ótimo significativamente quando está perto de um minimizador local. Já o Adam possui um caminho menos ruidoso e oscila menos em torno do mínimo local.\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='./images/comparison.png', width='1000px'>\n",
    "    Figura 4: SGD vs RMSProp cs Adam. Fonte: Aoron Defazio [3].\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Adam implementation\n",
    "def adam(x, y, w, b, alpha, num_iter):\n",
    "    # initialize the gradient\n",
    "    grad_w = np.zeros(w.shape)\n",
    "    grad_b = 0\n",
    "    # initialize the sum of square gradient\n",
    "    sum_w = np.zeros(w.shape)\n",
    "    sum_b = 0\n",
    "    # initialize the sum of gradient\n",
    "    sum_grad_w = np.zeros(w.shape)\n",
    "    sum_grad_b = 0\n",
    "    # initialize the loss\n",
    "    loss = np.zeros(num_iter)\n",
    "    for i in range(num_iter):\n",
    "        # calculate the gradient\n",
    "        grad_w = -2 * np.dot(x.T, (y - np.dot(x, w) - b))\n",
    "        grad_b = -2 * np.sum(y - np.dot(x, w) - b)\n",
    "        # calculate the sum of square gradient\n",
    "        sum_w += grad_w ** 2\n",
    "        sum_b += grad_b ** 2\n",
    "        # calculate the sum of gradient\n",
    "        sum_grad_w += grad_w\n",
    "        sum_grad_b += grad_b\n",
    "        # update the parameters\n",
    "        w = w - alpha * grad_w / (np.sqrt(sum_w / (i + 1)) + 1e-8)\n",
    "        b = b - alpha * grad_b / (np.sqrt(sum_b / (i + 1)) + 1e-8)\n",
    "        # calculate the loss\n",
    "        loss[i] = np.sum((y - np.dot(x, w) - b) ** 2) / x.shape[0]\n",
    "    return w, b, loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como sugestão para visualização do comportamento de vários tipos de gradientes ao longo do tempo, o site [Imgur - Visualizing Optimization Algos](https://imgur.com/a/Hqolp#NKsFHJb) possui uns gifs interessantes sobre o tema.\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='https://i.imgur.com/2dKCQHh.gif'>\n",
    "    Figura 5: Gif 1. Fonte: Imgur (2014) [6].\n",
    "</p>\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='https://i.imgur.com/pD0hWu5.gif'>\n",
    "    Figura 6: Gif 2. Fonte: Imgur (2014) [6].\n",
    "</p>\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='https://i.imgur.com/NKsFHJb.gif'>\n",
    "    Figura 7: Gif 1. Fonte: Imgur (2014) [6].\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências\n",
    "\n",
    "[[1](https://www.amazon.com.br/Deep-Learning-Python-Francois-Chollet/dp/1617294438)] François, C. Deep Learning with Python. Manning, 2018.\n",
    "\n",
    "[[2](https://jmlr.org/papers/v12/duchi11a.html)] Duchi, J. et al. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization, 2011.\n",
    "\n",
    "[[3](https://atcold.github.io/pytorch-Deep-Learning/pt/week05/05-2/)] Técnicas de Otimização II. Aoron Defazio. Disponível em: https://atcold.github.io/pytorch-Deep-Learning/pt/week05/05-2/. Acesso em 31 de mar de 2023.\n",
    "\n",
    "[[4](https://www.amazon.com.br/M%C3%A3os-Obra-Aprendizado-Scikit-Learn-Inteligentes-ebook/dp/B09H13N8FN/ref=sr_1_1?crid=B5JJOJJB2CBA&keywords=maos+a+obra+aprendizado+de+maquina&qid=1680314614&sprefix=maos+a+%2Caps%2C228&sr=8-1)] Géron, A. Mãos A Obra: Aprendizado De Máquina Com Scikit-Learn, Keras & TensorFlow: Conceitos, Ferramentas e Técnicas Para a Construção de Sistemas Inteligentes. O'reilly, 2021.\n",
    "\n",
    "[[5](https://www.amazon.com.br/Deep-Learning-Ian-Goodfellow/dp/0262035618/ref=sr_1_1?adgrpid=1141293728106820&hvadid=71331024008359&hvbmt=be&hvdev=c&hvlocphy=147031&hvnetw=o&hvqmt=e&hvtargid=kwd-71331371521175%3Aloc-20&hydadcr=5655_13210322&keywords=deep+learning+goodfellow&qid=1680315011&sr=8-1&ufe=app_do%3Aamzn1.fos.25548f35-0de7-44b3-b28e-0f56f3f96147)] Goodfellowm I. et al. Deep Learning. The Mit Press, 2016.\n",
    "\n",
    "[[6](https://imgur.com/a/Hqolp#NKsFHJb)] Imgur - Visualizing Optimization Algos. Disponível em: https://imgur.com/a/Hqolp#NKsFHJb. Acesso em 31 de mar de 2023.\n",
    "\n",
    "[[7](https://theberkeleyview.wordpress.com/2015/11/19/berkeleyview-for-adam-a-method-for-stochastic-optimization/#:~:text=ADAM%3A%20A%20Method%20for%20Stochastic%20Optimization.%20Adam%20is,using%20exponential%20moving%20average%2C%20and%20corrects%20its%20bias.)] Kingma, D; Ba, J. Adam: A Method for Stochastic Optimization, 2015.\n",
    "\n",
    "[[8](https://towardsdatascience.com/deep-learning-optimizers-436171c9e23f)]Mayanglambam, G. Deep Learning Optimizers. Disponível em: https://towardsdatascience.com/deep-learning-optimizers-436171c9e23f. Acesso em 31 de mar de 2023."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02b517450ae54cd96c8b705d566c285c501d3a4af01d206853f99222b3e7689f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
