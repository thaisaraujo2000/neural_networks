{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IvosLIk39Esa"
      },
      "source": [
        "# Questão 4"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "s7iWi0-T9Esc"
      },
      "source": [
        "Apresente um estudo sobre transferência de conhecimento no contexto de deep learning."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nWU6PVdf9Esc"
      },
      "source": [
        "## 1 - Como utilizar modelos pré-treinados e Transfer Learning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vVdFT3I29Esd"
      },
      "source": [
        "Modelos de **redes neurais convolucionais profundos** podem levar dias ou até semanas para serem treinados em conjuntos de dados muito grandes. Uma maneira de agilizar esse processo é reutilizar os pesos do modelo de modelos pré-treinados que foram desenvolvidos para conjuntos de dados de referência padrão em visão computacional, como as tarefas de reconhecimento de imagem do ImageNet. Os modelos de melhor desempenho podem ser baixados e usados diretamente ou integrados a um novo modelo para seus próprios problemas de visão computacional. Nesta lição, você descobrirá como usar a transferência de aprendizado ao desenvolver redes neurais convolucionais para aplicações de visão computacional. Depois de ler esta anotação, você saberá:\n",
        "\n",
        " - A transferência de aprendizado envolve o uso de modelos treinados em um problema como ponto de partida em um problema relacionado.\n",
        " - A transferência de aprendizado é flexível, permitindo o uso de modelos pré-treinados diretamente, como pré-processamento de extração de características e integrados a novos modelos completamente diferentes.\n",
        " - O Keras fornece acesso conveniente a muitos modelos de melhor desempenho nas tarefas de reconhecimento de imagem do ImageNet, como VGG, Inception e ResNet."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Principais benefícios do Transfer Learning](./images/transfer-learning.webp)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jC06xhH89Esd"
      },
      "source": [
        "### 1.1 - O que é o Transfer Learning?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4dyV1HDE9Esd"
      },
      "source": [
        "A **transferência de aprendizado** geralmente se refere a um processo em que um **modelo treinado em um problema é usado de alguma forma em um segundo problema relacionado**. No aprendizado profundo, a transferência de aprendizado é uma técnica em que um modelo de rede neural é primeiro treinado em um problema semelhante ao problema que está sendo resolvido. Uma ou mais camadas do modelo treinado são então usadas em um novo modelo treinado no problema de interesse.\n",
        "\n",
        "> Isso é geralmente entendido em um contexto de aprendizado supervisionado, onde a entrada é a mesma, mas o alvo pode ser de uma natureza diferente. Por exemplo, podemos aprender sobre um conjunto de categorias visuais, como gatos e cachorros, no primeiro cenário, e depois aprender sobre um conjunto diferente de categorias visuais, como formigas e vespas, no segundo cenário.\n",
        "\n",
        "A transferência de aprendizado tem o **benefício de reduzir o tempo de treinamento** de um modelo de rede neural e pode resultar em erro de generalização mais baixo. Os pesos nas camadas reutilizadas podem ser usados como ponto de partida para o processo de treinamento e adaptados em resposta ao novo problema. Esse uso trata a transferência de aprendizado como um tipo de esquema de inicialização de peso. Isso pode ser útil quando o primeiro problema relacionado tem muito mais dados rotulados do que o problema de interesse e a similaridade na estrutura do problema pode ser útil em ambos os contextos."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EMqn1UXh9Ese"
      },
      "source": [
        "### 1.2 - Como utilizar modelos pré-treinados"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KraM6dYZ9Ese"
      },
      "source": [
        "Uma variedade de modelos de alto desempenho foram desenvolvidos para classificação de imagens e demonstrados no desafio anual de reconhecimento visual em grande escala ImageNet, ou ILSVRC. Este desafio, frequentemente referido simplesmente como ImageNet, dado a fonte da imagem usada na competição, resultou em uma série de inovações na arquitetura e treinamento de redes neurais convolucionais. Além disso, muitos dos modelos usados nas competições foram disponibilizados sob uma licença permissiva. Esses modelos podem ser usados como base para transferência de aprendizado em aplicações de visão computacional. Isso é desejável por várias razões, não menos importantes:\n",
        "\n",
        "- **Recursos de Aprendizagem Úteis**: Os modelos aprenderam a detectar recursos genéricos em fotografias, considerando que foram treinados em mais de 1 milhão de imagens para 1.000 categorias.\n",
        "- **Desempenho de Ponta**: Os modelos alcançaram desempenho de ponta e permanecem eficazes na tarefa específica de reconhecimento de imagem para a qual foram desenvolvidos.\n",
        "- **Facilmente Acessível**: Os pesos do modelo são fornecidos como arquivos gratuitos para download e muitas bibliotecas fornecem APIs convenientes para baixar e usar os modelos diretamente.\n",
        "\n",
        "Os pesos do modelo podem ser baixados e usados na mesma arquitetura de modelo usando uma variedade de bibliotecas de aprendizado profundo diferentes, incluindo o Keras.\n",
        "\n",
        "**O uso de um modelo pré-treinado é limitado apenas pela sua criatividade**. Por exemplo, um modelo pode ser baixado e usado como está, como incorporado em um aplicativo e usado para classificar novas fotografias. Alternativamente, os modelos podem ser baixados e usados como modelos de extração de características. Aqui, a saída do modelo de uma camada anterior à camada de saída do modelo é usada como entrada para um novo modelo classificador. Lembre-se de que as camadas convolucionais mais próximas à camada de entrada do modelo aprendem recursos de baixo nível, como linhas, que as camadas no meio da camada aprendem recursos abstratos complexos que combinam os recursos de nível inferior extraídos da entrada, e as camadas mais próximas à saída interpretam os recursos extraídos no contexto de uma tarefa de classificação.\n",
        "\n",
        "Compreendendo isso, um nível de detalhe para extração de recursos de um modelo pré-treinado existente pode ser escolhido. Por exemplo, se uma nova tarefa for bastante diferente da classificação de objetos em fotografias (por exemplo, diferente do ImageNet), então talvez a saída do modelo pré-treinado após as primeiras camadas seja apropriada. Se uma nova tarefa for bastante similar à tarefa de classificar objetos em fotografias, então talvez a saída de camadas muito mais profundas no modelo possa ser usada, ou até mesmo a saída da camada totalmente conectada anterior à camada de saída possa ser usada.\n",
        "\n",
        "O modelo pré-treinado pode ser usado como um programa separado de extração de recursos, onde a entrada pode ser pré-processada pelo modelo ou por parte do modelo para obter uma saída (por exemplo, um vetor de números) para cada imagem de entrada, que pode então ser usado como entrada ao treinar um novo modelo. Alternativamente, o modelo pré-treinado ou a parte desejada do modelo podem ser integrados diretamente a um novo modelo de rede neural. Nesse caso, os pesos do modelo pré-treinado podem ser congelados para que não sejam atualizados durante o treinamento do novo modelo. Alternativamente, os pesos podem ser atualizados durante o treinamento do novo modelo, talvez com uma taxa de aprendizado mais baixa, permitindo que o modelo pré-treinado atue como um esquema de inicialização de pesos durante o treinamento do novo modelo. Podemos resumir alguns desses padrões de uso da seguinte forma:\n",
        "\n",
        "- **Classificador**: O modelo pré-treinado é usado diretamente para classificar novas imagens.\n",
        "- **Extrator de Features Standalone**: O modelo pré-treinado, ou alguma parte do modelo, é usado para pré-processar imagens e extrair recursos relevantes.\n",
        "- **Extrator de Features Integrado**: O modelo pré-treinado, ou alguma parte do modelo, é integrado a um novo modelo, mas as camadas do modelo pré-treinado são congeladas durante o treinamento.\n",
        "- **Inicialização de Pesos**: O modelo pré-treinado, ou alguma parte do modelo, é integrado a um novo modelo, e as camadas do modelo pré-treinado são treinadas em conjunto com o novo modelo.\n",
        "\n",
        "Cada abordagem pode ser eficaz e economizar tempo significativo no desenvolvimento e treinamento de um modelo de rede neural convolucional profundo. Pode não estar claro qual uso do modelo pré-treinado pode produzir os melhores resultados em sua nova tarefa de visão computacional, portanto, **alguma experimentação pode ser necessária**."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Diferença entre treinar do zero e usar Transfer Learning](./images/transfer_learning.jpeg)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zxBikyCM9Esf"
      },
      "source": [
        "## 2 - Extraindo features com um modelo CNN pré-treinado"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5kAVUTGp9Esf"
      },
      "source": [
        "Até este ponto, tratamos Redes Neurais Convolucionais como classificadores de imagens de ponta a ponta:\n",
        "\n",
        "1. Nós inserimos uma imagem na rede.\n",
        "2. A imagem é propagada para a frente pela rede.\n",
        "3. Obtemos as probabilidades de classificação finais a partir do final da rede.\n",
        "\n",
        "No entanto, não há uma \"regra\" que diga que devemos permitir que a imagem avance para frente por toda a rede.\n",
        "\n",
        "> Em vez disso, podemos interromper a propagação em uma camada arbitrária, como uma camada de ativação ou pooling, extrair os valores da rede neste momento e depois usá-los como vetores de características.\n",
        "\n",
        "Ao tratar as redes como um extrator de features, essencialmente \"cortamos\" a rede em um ponto arbitrário (normalmente antes das camadas totalmente conectadas, mas depende do seu conjunto de dados específico).\n",
        "\n",
        "Se repetirmos esse processo para um conjunto de dados inteiro de imagens, teremos uma matriz de design de N imagens, as quais podem ser usadas para quantificar seus conteúdos (ou seja, vetores de características). Com nossos vetores de características, podemos treinar um modelo de aprendizado de máquina pronto para uso, como um **classificador SVM linear, regressão logística ou floresta aleatória** com base nessas características para obter um classificador que reconheça novas classes de imagens.\n",
        "\n",
        "Lembre-se de que a própria CNN não é capaz de reconhecer essas novas classes - em vez disso, estamos usando a CNN como um extrator de características intermediário. O classificador downstream de aprendizado de máquina cuidará de aprender os padrões subjacentes das características extraídas da CNN."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ooza82j9Esf"
      },
      "source": [
        "## 3 - Fine-tuning da rede"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "av7HDAgO9Esf"
      },
      "source": [
        "Na seção anterior, aprendemos como tratar uma **Rede Neural Convolucional** pré-treinada como um **extrator de características**.\n",
        "\n",
        "> Usando esse extrator de características, propagamos para frente nosso conjunto de dados de imagens pela rede, extraímos as ativações em uma determinada camada e salvamos os valores em disco. Em seguida, um classificador de aprendizado de máquina raso foi treinado com base nas características da CNN.\n",
        "\n",
        "Essa abordagem de extrator de características da CNN, chamada de **transferência de aprendizado**, obteve uma precisão notável, muito superior a qualquer um de nossos experimentos anteriores no conjunto de dados Animals.\n",
        "\n",
        "Mas há outro tipo de transferência de aprendizado, um que pode realmente superar o método de extração de características se você tiver dados suficientes. Este método é chamado de **fine-tuning** e **requer que realizemos uma \"cirurgia na rede\"**.\n",
        "\n",
        "1. Primeiro, pegamos um **bisturi e cortamos o conjunto final de camadas totalmente conectadas** (ou seja, a \"cabeça\" da rede) de uma Rede Neural Convolucional pré-treinada, como VGG, ResNet, Inception, etc.\n",
        "2. Em seguida, **substituímos a cabeça** por um novo conjunto de camadas totalmente conectadas com inicializações aleatórias. A partir daí, todas as camadas abaixo da cabeça são congeladas para que seus pesos não possam ser atualizados (ou seja, o passe de trás na retropropagação não os alcança)\n",
        "3. Em seguida, treinamos a rede **usando uma taxa de aprendizado muito baixa** para que o novo conjunto de camadas FC possa começar a aprender padrões a partir das camadas CONV previamente aprendidas na rede.\n",
        "4. Opcionalmente, podemos descongelar o restante da rede e continuar o treinamento. Aplicar o fine-tuning permite aplicar redes pré-treinadas para reconhecer classes para as quais não foram originalmente treinadas; além disso, **este método pode levar a uma maior precisão do que a extração de características**."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Arquitetura da VGG16](./images/vgg16.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ktIym-679Esg"
      },
      "source": [
        "### 3.1 - Transfer Learning e Fine-Tuning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_O7mp9vX9Esg"
      },
      "source": [
        "\"**Fine-tuning é um tipo de transferência de aprendizado**. Aplicamos o fine-tuning em modelos de aprendizado profundo que já foram treinados em um determinado conjunto de dados. Tipicamente, essas redes são arquiteturas de ponta, como VGG, ResNet e Inception, que foram treinadas no conjunto de dados ImageNet.\n",
        "\n",
        "Como descobrimos na seção anterior sobre extração de características, essas redes contêm filtros discriminativos ricos que podem ser usados em conjuntos de dados e rótulos de classe fora daqueles em que já foram treinados. No entanto, em vez de simplesmente aplicar a extração de características, vamos realizar uma cirurgia na rede e modificar a arquitetura real para que possamos re-treinar partes da rede.\n",
        "\n",
        "Na maioria dos casos, a nova cabeça FC terá menos parâmetros do que a original; no entanto, isso realmente depende do seu conjunto de dados específico. A nova cabeça FC é inicializada aleatoriamente (como qualquer outra camada em uma nova rede) e conectada ao corpo da rede original, e estamos prontos para treinar.\n",
        "\n",
        "No entanto, há um problema - nossas camadas convolucionais já aprenderam filtros discriminativos e ricos, enquanto nossas camadas FC são totalmente novas e aleatórias. Se permitirmos que o gradiente retropropague a partir desses valores aleatórios por todo o corpo da rede, corremos o risco de destruir esses recursos poderosos. Para contornar isso, em vez disso, permitimos que nossa cabeça FC \"aqueça\" (ironicamente) \"congelando\" todas as camadas do corpo da rede, como na Figura abaixo (esquerda).\n",
        "\n",
        "Esquerda: Quando iniciamos o processo de ajuste fino, congelamos todas as camadas CONV da rede e só permitimos que o gradiente retroceda pelas camadas FC. Isso permite que nossa rede se \"aqueça\". Direita: Depois que as camadas FC tiverem a chance de se aquecer, podemos optar por descongelar todas as camadas da rede e permitir que cada uma delas também seja ajustada.\n",
        "\n",
        "Os dados de treinamento são propagados através da rede como normalmente faríamos; no entanto, a retropropagação é interrompida após as camadas FC, o que permite que essas camadas comecem a aprender padrões das camadas CONV altamente discriminativas. Em alguns casos, nunca desbloqueamos o corpo da rede, pois nosso novo FC head pode obter precisão suficiente.\n",
        "\n",
        "No entanto, para alguns conjuntos de dados, é frequentemente vantajoso permitir que as camadas CONV originais sejam modificadas durante o processo de ajuste fino (Figura acima, à direita).\n",
        "\n",
        "Depois que o FC head começar a aprender padrões em nosso conjunto de dados, pause o treinamento, descongele o corpo e continue o treinamento, mas com uma taxa de aprendizado muito pequena - não queremos desviar muito nossos filtros CONV.\n",
        "\n",
        "Em seguida, o treinamento pode continuar até que seja obtida precisão suficiente. O ajuste fino é um método extremamente avançado para obter classificadores de imagens a partir de CNNs pré-treinadas em conjuntos de dados personalizados, ainda mais avançado do que a extração de recursos na maioria dos casos. **A desvantagem é que o ajuste fino pode exigir um pouco mais de trabalho e a escolha dos parâmetros da cabeça FC desempenha um papel importante na precisão da rede** - não é possível confiar estritamente nas técnicas de regularização aqui, pois a rede já foi pré-treinada e você não pode se desviar da regularização que já está sendo executada pela rede.\n",
        "\n",
        "Em segundo lugar, para conjuntos de dados pequenos, pode ser desafiador fazer com que sua rede comece a \"aprender\" a partir de um início \"frio\" da FC, e é por isso que congelamos o corpo da rede primeiro. Mesmo assim, passar do estágio de aquecimento pode ser um pouco desafiador e pode exigir o uso de outros otimizadores além do SGD. **Embora o ajuste fino exija um pouco mais de esforço, se for feito corretamente, você quase sempre terá maior precisão**."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
